# Data Engineering Portfolio Showcase

This project is an interactive web application built with Streamlit to demonstrate two data engineering projects. It showcases skills in real-time data processing, cloud architecture, and data warehousing.

## ğŸš€ Features

- **Interactive Dashboards:** Two distinct dashboards for visualizing data from different data pipelines.
- **Real-time Data:** Data is fetched live from AWS Redshift and Snowflake, with a 15-minute cache.
- **AWS + OpenFDA Pipeline:** A real-time data pipeline that ingests, processes, and visualizes drug adverse event data from the OpenFDA API.
- **Snowflake + Synthea Pipeline:** A data warehousing project that processes and analyzes synthetic healthcare insurance data generated by Synthea.

## ğŸ—ï¸ Architecture

### 1. AWS + OpenFDA Real-time Pipeline

This pipeline demonstrates a serverless, real-time data ingestion and processing system on AWS.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open FDA API   â”‚â”€â”€â”€â–¶â”‚  Data Ingestion â”‚â”€â”€â”€â–¶â”‚  Kinesis Stream â”‚
â”‚                 â”‚    â”‚   (Lambda)      â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Analysis â”‚â—€â”€â”€â”€â”‚ Data Processing â”‚â”€â”€â”€â–¶â”‚   Data Storage  â”‚
â”‚(This Dashboard) â”‚    â”‚   (Lambda)      â”‚    â”‚   (Firehose +   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚    Redshift)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Snowflake + Synthea Pipeline

This pipeline showcases a modern data warehousing architecture using Snowflake.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Source â”‚â”€â”€â”€â–¶â”‚    Raw Layer    â”‚â”€â”€â”€â–¶â”‚ Processed Layer â”‚â”€â”€â”€â–¶â”‚  Analytics Layer â”‚
â”‚    (Synthea)  â”‚    â”‚ (S3, Snowpipe)  â”‚    â”‚ (Fact/Dimension)â”‚    â”‚      (Views)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                          â”‚
                                                                          â–¼
                                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                   â”‚ Presentation   â”‚
                                                                   â”‚     Layer      â”‚
                                                                   â”‚   (Streamlit)  â”‚
                                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

- **Frontend:** Streamlit
- **Backend & Data Analysis:** Python, Pandas, Plotly
- **Databases:**
  - AWS Redshift
  - Snowflake
- **Cloud Services (AWS):**
  - AWS Lambda
  - AWS Kinesis (Data Streams & Firehose)
  - AWS S3
  - AWS DynamoDB
- **Cloud Services (Snowflake):**
  - Snowpipe
- **Deployment:** Streamlit Community Cloud

## ğŸƒâ€â™€ï¸ Running the Project Locally

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Yassir2026/Streamlit-Repo.git
   cd Streamlit-Repo
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up database connections:**
   This project requires connections to AWS Redshift and Snowflake. You will need to configure your credentials to run the application with live data.

   Create a file `.streamlit/secrets.toml` and add your connection details. You can use the following template:

   ```toml
   # Redshift Connection
   [redshift]
   host = "your-redshift-host"
   port = 5439
   database = "your-database"
   user = "your-user"
   password = "your-password"

   # Snowflake Connection
   [snowflake]
   user = "your-user"
   password = "your-password"
   account = "your-account"
   warehouse = "your-warehouse"
   database = "your-database"
   schema = "your-schema"
   ```
   The connection logic is in `database_connections/redshift_connection.py` and `database_connections/snowflake_connection.py`. You may need to adjust the code to fit your credential management strategy.

   **Note:** If database connections are not available, the application will fall back to using mock data.

4. **Run the Streamlit app:**
   ```bash
   streamlit run app.py
   ```

## ğŸŒ Deployment

This application is deployed on [Streamlit Community Cloud](https://streamlit.io/cloud). The deployment is configured to connect to the GitHub repository and automatically redeploys on new commits to the `main` branch.
